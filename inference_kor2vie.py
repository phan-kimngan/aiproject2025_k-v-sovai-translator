from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
from datasets import load_dataset, DatasetDict
import torch, json, pandas as pd, re
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
import torch, gc
torch.cuda.empty_cache()
gc.collect()

model_path = "./skt-ax3.1light-kor-vie-qlora"

tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"


model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    offload_folder="./offload",        # üí° th√™m d√≤ng n√†y
    low_cpu_mem_usage=True             # tu·ª≥ ch·ªçn
)

model.config.pad_token_id = model.config.eos_token_id
model.eval()


dataset = DatasetDict({
    "train": load_dataset("json", data_files="Advanced AI Project/train.json")["train"],
    "val":   load_dataset("json", data_files="Advanced AI Project/val.json")["train"],
    "test":  load_dataset("json", data_files="Advanced AI Project/test.json")["train"],
})
test_data = dataset["test"]

batch_size = 8
results = []

for i in tqdm(range(0, len(test_data), batch_size), desc="Generating translations (clean)"):
    batch = test_data[i:i + batch_size]
    batch_korean = batch["input"]
    batch_refs = batch["output"]

    # T·∫°o prompt ƒë√∫ng chu·∫©n LLaMA-3
    batch_prompts = []
    for text in batch_korean:
        messages = [
            {"role": "system", "content": "You are a helpful translator that translates Korean to Vietnamese."},
            {"role": "user", "content": f"Translate the following Korean sentence into Vietnamese:\n\n{text}"},
        ]
        batch_prompts.append(
            tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        )

    # Tokenize
    inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=80,
            temperature=0.3,
            top_p=0.8,
            do_sample=False,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )

    attn = inputs["attention_mask"]
    input_lengths = attn.sum(dim=1).tolist()

    for seq, in_len, src, ref in zip(outputs, input_lengths, batch_korean, batch_refs):
        gen_ids = seq[in_len:]    
        

        import unicodedata

        generated = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()
        

        generated = unicodedata.normalize("NFKC", generated)

        generated = re.sub(
            r"(?is)(translate\s+the\s+following\s+korean\s+sentence\s+into\s+vietnamese[:Ôºö]?\s*|"
            r"you\s+are\s+a\s+helpful\s+translator\s+that\s+translates[:Ôºö]?\s*|"
            r"are\s+a\s+helpful\s+translator\s+that\s+translates[:Ôºö]?\s*|"
            r"that\s+translates[:Ôºö]?\s*|"
            r"to\s+vietnamese[:Ôºö]?\s*|"
            r"vietnamese[:Ôºö]?\s*|"
            r"sentence\s+in[:Ôºö]?\s*|"
            r"in[:Ôºö]?\s*|"
            r"following\s+korean\s+sentence\s+into\s+vietnamese[:Ôºö]?\s*|"
            r"user[:Ôºö]?|assistant[:Ôºö]?|input[:Ôºö]?|output[:Ôºö]?|korean[:Ôºö]?|english[:Ôºö]?|the\s+)?",
            "",
            generated,
        )


        generated = generated.replace("\ufeff", "")
        generated = re.sub(r"[\u200B-\u200D\uFEFF\u00A0]+", "", generated)  # zero-width, NBSP
        generated = re.sub(r"[Í∞Ä-Ìû£„Ñ±-„Öé„Öè-„Ö£ÔøΩ]+", "", generated)            # n·∫øu c√≤n s√≥t H√†n + k√Ω t·ª± l·ªói

        generated = re.sub(r"[\r\n\t]+", " ", generated)
        generated = re.sub(r"\s+", " ", generated).strip()

        patterns_head = [
            r"^\s*(?:\d+(?:\s*[,Ôºå]\s*\d+)*)(?:[\.\u3002,Ôºå:Ôºö;\)\]-‚Äì‚Äî])\s*",
            r"^(?:\d+(?:[\.\u3002,Ôºå:Ôºö;\)\]-‚Äì‚Äî])\s*)+",
            r"^\s*\d+[\.\u3002:Ôºö;\)\]-‚Äì‚Äî]+",
            r"^[\.\u3002,Ôºå:Ôºö;!\?‚Ä¶¬∑‚Ä¢\(\)\[\]\{\}\-‚Äì‚Äî\"‚Äú‚Äù\'\s]+",
            r"^(?:korean|the|english|sentence|input|output|are|a|helpful|translator|that|translates)\b[\s\.\,:\-‚Äì‚Äî;!\?\"‚Äú‚Äù\'\)]*",
        ]
        for _ in range(5):
            old = generated
            for pat in patterns_head:
                generated = re.sub(pat, "", generated, flags=re.IGNORECASE)
            generated = generated.lstrip()
            if generated == old:
                break

        generated = re.sub(r"\s+", " ", generated).strip()

        if generated:
            generated = generated[0].upper() + generated[1:]

        

        print(f"src: {src}")
        print(f"reference_vietnamese: {ref}")
        print(f"predicted_vietnamese: {generated}")
        print("\n")
        results.append({
            "korean": src,
            "reference_vietnamese": ref,
            "predicted_vietnamese": generated
        })

# ===========================
# 4Ô∏è‚É£ L∆∞u k·∫øt qu·∫£
# ===========================
output_json = "test_kor2vie.json"
output_csv = "test_kor2vie.csv"

with open(output_json, "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)
pd.DataFrame(results).to_csv(output_csv, index=False, encoding="utf-8-sig")

print(f"\nƒê√£ l∆∞u k·∫øt qu·∫£ inference:")
print(f"   - JSON: {output_json}")
print(f"   - CSV:  {output_csv}")


